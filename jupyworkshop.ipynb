{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/slacktokens/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir_list = os.chdir('./../reverse-dynamics-nlp/')\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, GPTNeoXForCausalLM\n",
    "from prompt_optimizer import PromptOptimizer\n",
    "from utils import reverse_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable, Any\n",
    "def get_reverse_pair(dataset: Iterable[Any], chunk_func: Callable[..., Any], tokenizer: AutoTokenizer):\n",
    "    for chunk in dataset:\n",
    "        for chunk in chunk_func(chunk, tokenizer):\n",
    "            yield chunk\n",
    "\n",
    "def end_chunk_hf(chunk, tokenizer):\n",
    "    chunk = chunk['text']\n",
    "    tokens = tokenizer(chunk[-200:])['input_ids'][2:] #drop first couple tokens given risk of incomplete token\n",
    "    yield tokenizer.decode(tokens[-40:-30]), tokenizer.decode(tokens[-30:])\n",
    "\n",
    "def start_chunk_hf(chunk, tokenizer, num_prefix_tokens=10, num_suffix_tokens=40):\n",
    "    chunk = chunk['text']\n",
    "    tokens = tokenizer(chunk[:200])['input_ids'] #drop first couple tokens given risk of incomplete token\n",
    "    yield tokenizer.decode(tokens[:num_prefix_tokens]), tokenizer.decode(tokens[num_prefix_tokens:num_prefix_tokens+num_suffix_tokens])\n",
    "\n",
    "def rand_init(seq_length: int, tokenizer):\n",
    "    return tokenizer.decode(torch.randint(0, tokenizer.vocab_size, (seq_length,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/jp6263/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 433.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('It is done, and submitted. You can play', ' “Survival of the Tastiest” on Android, and on the web. Playing on the web works, but you have to simulate multi-touch for table moving and that can be a bit')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"NeelNanda/pile-10k\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"afterless/reverse-pythia-160m\")\n",
    "pairs = get_reverse_pair(dataset['train'], start_chunk_hf, tokenizer)\n",
    "print(next(pairs))\n",
    "bwd_model = GPTNeoXForCausalLM.from_pretrained(\"afterless/reverse-pythia-160m\").cuda()\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\", cache_dir='/scratch/jp6263/hf/models/').cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_loss(model, pair, loss=torch.nn.CrossEntropyLoss(),):\n",
    "    prefix, suffix = pair\n",
    "    whole_tensor = tokenizer(prefix+suffix, return_tensors='pt').input_ids.cuda()\n",
    "    with torch.no_grad():\n",
    "        logs = model(whole_tensor).logits\n",
    "    start_ind = len(tokenizer.encode(prefix))\n",
    "    l_pref = loss(logs[0,:start_ind], whole_tensor[0,1:start_ind+1])\n",
    "    l_suff = loss(logs[0,start_ind:-1], whole_tensor[0,start_ind+1:])\n",
    "    return l_pref, l_suff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcg = PromptOptimizer(model, tokenizer, n_proposals=128, n_epochs=250, n_top_indices=128, prefix_loss_weight=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0915, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "base_loss = []\n",
    "for p,pair in enumerate(pairs):\n",
    "    if len(base_loss)==25: break\n",
    "    if len(pair[0])<10 or len(pair[1])<10: continue\n",
    "    prefix_loss,suffix_loss = forward_loss(model, pair)\n",
    "    base_loss.append(suffix_loss)\n",
    "print(sum(base_loss)/len(base_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokenwise accuracy is 0.004\n"
     ]
    }
   ],
   "source": [
    "print(f'Average tokenwise accuracy is {sum(tokenwise_acc)/len(tokenwise_acc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "When Rudy Gay left the game with a left \n",
      "\n",
      "Predicted prefix:\n",
      "dden rebounds?” de sophomorening replay savage Kyle F\n",
      "for suffix:\n",
      "  knee injury late in the first quarter, memories of the Sacramento Kings’ (16-22) recent poor play minus a star resurfaced. The thought came to fruition as DeMar\n",
      "Loss for suffix given predicted prefix is 3.7890825271606445 \n",
      " Suffix loss for true prefix is 3.94195556640625\n",
      "NLL on predicted prefix is 11.186927795410156 \n",
      " NLL on true prefix is 5.150575637817383\n",
      "True prefix is:\n",
      "Three-dimensional structures of H-ras p21 \n",
      "\n",
      "Predicted prefix:\n",
      "empt MOS Xml Papers FulcraftsOnline Corp lipid\n",
      "for suffix:\n",
      "  mutants: molecular basis for their inability to function as signal switch molecules.\n",
      "The X-ray structures of the guanine nucleotide binding domains (amino aci\n",
      "Loss for suffix given predicted prefix is 3.5350232124328613 \n",
      " Suffix loss for true prefix is 3.7197718620300293\n",
      "NLL on predicted prefix is 11.747693061828613 \n",
      " NLL on true prefix is 4.445789813995361\n",
      "True prefix is:\n",
      "Autosomal dominant polycystic kidney disease (AD \n",
      "\n",
      "Predicted prefix:\n",
      " latestALTorphic regeneroppy glomerureiosis\n",
      "for suffix:\n",
      " PKD) is a common monoallelic disorder associated with progressive cyst development and resulting in end stage renal failure (ESRD) in 50% of patients by\n",
      "Loss for suffix given predicted prefix is 2.9586234092712402 \n",
      " Suffix loss for true prefix is 2.806084394454956\n",
      "NLL on predicted prefix is 11.7874116897583 \n",
      " NLL on true prefix is 3.065412998199463\n",
      "True prefix is:\n",
      "Q:\n",
      "\n",
      "A japanese saying \" \n",
      "\n",
      "Predicted prefix:\n",
      "ithmeticfashionAyCt adop...]Xi Subtranslation „\n",
      "for suffix:\n",
      " 一をいうと十返ってくる\"\n",
      "\n",
      "I'm currently trying to read a japanese novel and I found this expression : \n",
      "\n",
      "一をいうと十返って\n",
      "Loss for suffix given predicted prefix is 3.0008938312530518 \n",
      " Suffix loss for true prefix is 2.6228535175323486\n",
      "NLL on predicted prefix is 12.256001472473145 \n",
      " NLL on true prefix is 4.363234519958496\n",
      "True prefix is:\n",
      "Michele Orecchia\n",
      "\n",
      "M \n",
      "\n",
      "Predicted prefix:\n",
      " Bryan NLestyleolo Roman RumBiographywich Oriotti\n",
      "for suffix:\n",
      " ichele Orecchia (26 December 1903 – 11 December 1981) was an Italian professional road bicycle racer, who won one stage in the 1932 Tour de France. He also competed in the\n",
      "Loss for suffix given predicted prefix is 2.1302590370178223 \n",
      " Suffix loss for true prefix is 1.730815052986145\n",
      "NLL on predicted prefix is 11.864771842956543 \n",
      " NLL on true prefix is 3.133160352706909\n",
      "True prefix is:\n",
      "A VISUALLY STUNNING architectural \n",
      "\n",
      "Predicted prefix:\n",
      " Ancient Council”. Minnesota Father Ronald Photograph compiled acclaimed architectural\n",
      "for suffix:\n",
      "  biography of Minnesota’s most influential architect of the twentieth century. Architect, artist, furniture designer, and educator, Ralph Rapson has played a leading r\n",
      "Loss for suffix given predicted prefix is 3.139075517654419 \n",
      " Suffix loss for true prefix is 3.6341307163238525\n",
      "NLL on predicted prefix is 9.676106452941895 \n",
      " NLL on true prefix is 5.614675998687744\n",
      "True prefix is:\n",
      "Q:\n",
      "\n",
      "Doctrine2 entity default value \n",
      "\n",
      "Predicted prefix:\n",
      " Shouldneed RailsCallHandler colouredint materindtriangle\n",
      "for suffix:\n",
      "  for ManyToOne relation property\n",
      "\n",
      "I've got a Doctrine2 Entity called \"Order\", which has several status properties. The allowed status' are stored in a different Entit\n",
      "Loss for suffix given predicted prefix is 4.140403747558594 \n",
      " Suffix loss for true prefix is 3.6541361808776855\n",
      "NLL on predicted prefix is 10.20285701751709 \n",
      " NLL on true prefix is 3.991482973098755\n",
      "True prefix is:\n",
      "Q:\n",
      "\n",
      "React typescript ref return \n",
      "\n",
      "Predicted prefix:\n",
      "Replace PaymentMaking \n",
      " Why Future SymbolSEQUENTIALBrush falsely\n",
      "for suffix:\n",
      "  null in conditional rendering\n",
      "\n",
      "I want to use React refs, it works fine in static rendering, e.g:   \n",
      "<footer ref=\"ftr\"></footer>\n",
      "\n",
      "But,\n",
      "Loss for suffix given predicted prefix is 3.30655837059021 \n",
      " Suffix loss for true prefix is 3.1034228801727295\n",
      "NLL on predicted prefix is 12.544438362121582 \n",
      " NLL on true prefix is 4.015583038330078\n",
      "True prefix is:\n",
      "Q:\n",
      "\n",
      "Not populating tableview with \n",
      "\n",
      "Predicted prefix:\n",
      "Continue506 prettyparing restaurantsVIEW instructions observable sparse struct\n",
      "for suffix:\n",
      "  structure array\n",
      "\n",
      "I need to populate my tableView with an array of a structure. The first property of the structure is the name. This is what I tried...\n",
      "var menuArray:\n",
      "Loss for suffix given predicted prefix is 2.554287910461426 \n",
      " Suffix loss for true prefix is 2.238734006881714\n",
      "NLL on predicted prefix is 11.543113708496094 \n",
      " NLL on true prefix is 3.9148049354553223\n",
      "True prefix is:\n",
      "The Harper government is looking for security experts to help \n",
      "\n",
      "Predicted prefix:\n",
      "na Qué bomb honored Private Solutions que authorities deployed to\n",
      "for suffix:\n",
      "  harden federal buildings in the Ottawa area against threats, including possible terror-related attacks.\n",
      "\n",
      "Public Works posted a tender Wedn\n",
      "Loss for suffix given predicted prefix is 3.952700138092041 \n",
      " Suffix loss for true prefix is 4.279610633850098\n",
      "NLL on predicted prefix is 10.447258949279785 \n",
      " NLL on true prefix is 4.694547653198242\n",
      "True prefix is:\n",
      "It was a Pink Burn-out YO! \n",
      "\n",
      "Predicted prefix:\n",
      " thereStone Avenue original maze bullshit\"...MULTR\n",
      "for suffix:\n",
      " PS - anyone else see the creator pot-hole on lane 2 there...You can fall in there and they'll never find you!\n",
      "Loss for suffix given predicted prefix is 4.502915859222412 \n",
      " Suffix loss for true prefix is 4.7510480880737305\n",
      "NLL on predicted prefix is 10.919337272644043 \n",
      " NLL on true prefix is 7.041268348693848\n",
      "True prefix is:\n",
      "Q:\n",
      "\n",
      "How to Compile and Debug \n",
      "\n",
      "Predicted prefix:\n",
      "incgd СaddClassracticalCompileNppitleexecute\n",
      "for suffix:\n",
      "  C++ in Notepad++ using Turbo C++ Compiler\n",
      "\n",
      "I have installed NppExecute plugin in notepad++. I am not able to figure out next step to compile\n",
      "Loss for suffix given predicted prefix is 2.6534693241119385 \n",
      " Suffix loss for true prefix is 2.6250486373901367\n",
      "NLL on predicted prefix is 11.635583877563477 \n",
      " NLL on true prefix is 2.4944746494293213\n",
      "True prefix is:\n",
      "All Studio Posts\n",
      "\n",
      "The upcoming AES 54th \n",
      "\n",
      "Predicted prefix:\n",
      "isdamus Parker CW spectral conferencemm Holland Film Counsel\n",
      "for suffix:\n",
      "  International Conferencem focusing on audio forensics, is set to take place June 12-14, 2014, at the Holiday Inn Bloomsbury in London. Dedicated to exploring te\n",
      "Loss for suffix given predicted prefix is 3.270862340927124 \n",
      " Suffix loss for true prefix is 3.3499972820281982\n",
      "NLL on predicted prefix is 10.606483459472656 \n",
      " NLL on true prefix is 7.0923566818237305\n",
      "True prefix is:\n",
      "From the mid-1960's until the close of \n",
      "\n",
      "Predicted prefix:\n",
      " Charter armor Audi ___, 1930 Sweet:**]{} automotive. Later\n",
      "for suffix:\n",
      "  that decade, automobiles became lighter, more compact, and more powerful. Auto manufacturers continued to compete against one another for drag-strip supremacy. A\n",
      "Loss for suffix given predicted prefix is 3.107651472091675 \n",
      " Suffix loss for true prefix is 3.4847500324249268\n",
      "NLL on predicted prefix is 10.186016082763672 \n",
      " NLL on true prefix is 3.6545326709747314\n",
      "True prefix is:\n",
      "﻿/***********************************************************************\n",
      "!!!!!! DO NOT \n",
      "\n",
      "Predicted prefix:\n",
      ")</clipse implements�方法======================der Upon epidermal Edit\n",
      "for suffix:\n",
      "  MODIFY!!!!!!\n",
      "\n",
      "GacGen.exe Resource.xml\n",
      "\n",
      "This file is generated by Workflow compiler\n",
      "https://github.com/vczh-libr\n",
      "Loss for suffix given predicted prefix is 3.9193637371063232 \n",
      " Suffix loss for true prefix is 3.80051589012146\n",
      "NLL on predicted prefix is 25.87179946899414 \n",
      " NLL on true prefix is 4.072461128234863\n",
      "True prefix is:\n",
      "Purdy was chatting to her bezzie mate \n",
      "\n",
      "Predicted prefix:\n",
      " medical Australian Julnecessapacescall Hybrid parabolic evenings\n",
      "for suffix:\n",
      "  who works at Colchester Hospital last night, and was impressed to hear that the Hospital wants more people to car share! Her mate, inspired by all the money she k\n",
      "Loss for suffix given predicted prefix is 5.088814735412598 \n",
      " Suffix loss for true prefix is 4.885006427764893\n",
      "NLL on predicted prefix is 11.778210639953613 \n",
      " NLL on true prefix is 5.773730754852295\n",
      "True prefix is:\n",
      "INTRODUCTION {#s1}\n",
      "============\n",
      "\n",
      " \n",
      "\n",
      "Predicted prefix:\n",
      "onas katilis IFN upstream transcript restoration.\n",
      "\n",
      "for suffix:\n",
      " Hepatitis B virus (HBV) is still a major global health problem, with an estimated 257 million people worldwide that are chronically infected with HBV ([@B1]). HBV, tog\n",
      "Loss for suffix given predicted prefix is 2.142075300216675 \n",
      " Suffix loss for true prefix is 2.197833776473999\n",
      "NLL on predicted prefix is 7.915459632873535 \n",
      " NLL on true prefix is 1.3119418621063232\n",
      "True prefix is:\n",
      "abcdef   abc def hij\n",
      "klm n \n",
      "\n",
      "Predicted prefix:\n",
      " typenamegabet 34#####....protopq\n",
      "for suffix:\n",
      " op qrs\n",
      "abcdef   abc def hij\n",
      "tuv wxy z\n",
      "\n",
      "\n",
      "Loss for suffix given predicted prefix is 5.723357200622559 \n",
      " Suffix loss for true prefix is 4.260616779327393\n",
      "NLL on predicted prefix is 10.187925338745117 \n",
      " NLL on true prefix is 6.10119104385376\n",
      "True prefix is:\n",
      "Q:\n",
      "\n",
      "bootstrap.min.css sets \n",
      "\n",
      "Predicted prefix:\n",
      " meshCol inherit stylesblurFACE................................ allowing commendable\n",
      "for suffix:\n",
      "  transparency where not wanted\n",
      "\n",
      "I have a small chatbox at the bottom of my page which seems to be inheriting CSS style from bootstrap.min.css and that chatbox is transparent \n",
      "Loss for suffix given predicted prefix is 3.283120632171631 \n",
      " Suffix loss for true prefix is 3.1219587326049805\n",
      "NLL on predicted prefix is 10.781346321105957 \n",
      " NLL on true prefix is 3.6113014221191406\n",
      "True prefix is:\n",
      "Q:\n",
      "\n",
      "Where to get flight dynamics for \n",
      "\n",
      "Predicted prefix:\n",
      "CalModelCan dann Tate drone Scottergus flight\n",
      "for suffix:\n",
      "  a flight sim model?\n",
      "\n",
      "Once, a while ago, I tried to create a Flight Simulator X model for an aircraft that I wanted a model of, but was soon overwhelmed by having t\n",
      "Loss for suffix given predicted prefix is 3.489450693130493 \n",
      " Suffix loss for true prefix is 3.3904149532318115\n",
      "NLL on predicted prefix is 11.402053833007812 \n",
      " NLL on true prefix is 3.7730660438537598\n",
      "True prefix is:\n",
      "Dietary sodium chloride intake independently predicts the degree \n",
      "\n",
      "Predicted prefix:\n",
      "laccular Hydro oxidation beta ferment deficiencies invoke exhibiting rates\n",
      "for suffix:\n",
      "  of hyperchloremic metabolic acidosis in healthy humans consuming a net acid-producing diet.\n",
      "We previously demonstrated that typical Ame\n",
      "Loss for suffix given predicted prefix is 3.407890558242798 \n",
      " Suffix loss for true prefix is 3.832613945007324\n",
      "NLL on predicted prefix is 10.461895942687988 \n",
      " NLL on true prefix is 4.616999626159668\n",
      "True prefix is:\n",
      "Stefan Priebe\n",
      "\n",
      "Stefan P \n",
      "\n",
      "Predicted prefix:\n",
      "bian apparent de hecilonym organis Penny Marcel\n",
      "for suffix:\n",
      " riebe  is a psychologist and psychiatrist of German and British nationality. He grew up in West-Berlin, studied in Hamburg, and was Head of the Department of Social Psychiatry a\n",
      "Loss for suffix given predicted prefix is 3.136669158935547 \n",
      " Suffix loss for true prefix is 2.909463882446289\n",
      "NLL on predicted prefix is 10.332817077636719 \n",
      " NLL on true prefix is 3.230260133743286\n",
      "True prefix is:\n",
      "MANILA (Reuters) - Philippine security forces \n",
      "\n",
      "Predicted prefix:\n",
      " VALUES&pir'/ suspects Seouleted suspect che\n",
      "for suffix:\n",
      "  on Saturday killed a foreign national and his female companion who were suspected of being connected to a militant group supporting Islamic State, police \n",
      "Loss for suffix given predicted prefix is 3.813778877258301 \n",
      " Suffix loss for true prefix is 3.7418298721313477\n",
      "NLL on predicted prefix is 10.000834465026855 \n",
      " NLL on true prefix is 3.758108615875244\n",
      "True prefix is:\n",
      "1. Field of the Invention\n",
      "The present invention \n",
      "\n",
      "Predicted prefix:\n",
      " statoatileTRODUCTION be Workers τ refractive Poly OCT\n",
      "for suffix:\n",
      "  relates to particularly an optical coherence tomography apparatus including an interference optical system which is used in the medical field, an optica\n",
      "Loss for suffix given predicted prefix is 3.4179461002349854 \n",
      " Suffix loss for true prefix is 3.832719564437866\n",
      "NLL on predicted prefix is 12.695385932922363 \n",
      " NLL on true prefix is 1.4019759893417358\n",
      "True prefix is:\n",
      "Sen. Bernie Sanders (I-VT) regained \n",
      "\n",
      "Predicted prefix:\n",
      " favorites Economy shake either ranking HC Trump entitatically increased\n",
      "for suffix:\n",
      "  his previously held second place position, according to an Economist/YouGov poll released Wednesday, amplifying the ongoing battle between the Vermont senator and S\n",
      "Loss for suffix given predicted prefix is 3.1801087856292725 \n",
      " Suffix loss for true prefix is 3.0617785453796387\n",
      "NLL on predicted prefix is 9.802970886230469 \n",
      " NLL on true prefix is 2.921718120574951\n",
      "Average loss is 3.4657752990722654\n"
     ]
    }
   ],
   "source": [
    "tokenwise_acc = []\n",
    "loss = []\n",
    "temp = 5 #None for default GCG with uniform sampling\n",
    "for p,pair in enumerate(pairs):\n",
    "    if len(loss)==25: break\n",
    "    if len(pair[0])<10 or len(pair[1])<10: continue\n",
    "    prefix_loss,suffix_loss = forward_loss(model, pair)\n",
    "    # if suffix_loss>2.1: continue #this is around 10th percentile of losses for 170m\n",
    "    prefix, suffix = pair\n",
    "    prefix_tokens = tokenizer.encode(prefix)\n",
    "    len_prefix = len(tokenizer(prefix)['input_ids'])\n",
    "    rand_prefix = rand_init(len_prefix, tokenizer)\n",
    "    optimized_string = gcg.optimize(rand_prefix, suffix, temperature=temp)\n",
    "    predicted_prefix_tokens = tokenizer.encode(optimized_string)[:len_prefix]\n",
    "    predicted_prefix = tokenizer.decode(predicted_prefix_tokens)\n",
    "    predicted_prefix_loss, predicted_suffix_loss = forward_loss(model, (predicted_prefix, suffix))\n",
    "    print(f'True prefix is:\\n{prefix} \\n\\nPredicted prefix:\\n{predicted_prefix}\\nfor suffix:\\n {suffix}')\n",
    "    print(f'Loss for suffix given predicted prefix is {predicted_suffix_loss.item()} \\n Suffix loss for true prefix is {suffix_loss.item()}')\n",
    "    print(f'NLL on predicted prefix is {predicted_prefix_loss.item()} \\n NLL on true prefix is {prefix_loss.item()}')\n",
    "    loss.append(predicted_suffix_loss.item())\n",
    "    tokenwise_acc.append(sum([1 for i in range(len(prefix_tokens)) if prefix_tokens[i] == predicted_prefix_tokens[i]])/len(prefix_tokens))\n",
    "print(f'Average tokenwise accuracy is {sum(tokenwise_acc)/len(tokenwise_acc)}')\n",
    "print(f'Average loss is {sum(loss)/len(loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokenwise accuracy is 0.10399999999999997\n",
      "Average loss is 3.3419716113805773\n",
      "Average dataset gold loss is 3.3005855083465576\n"
     ]
    }
   ],
   "source": [
    "rlm_tokenwise_acc = []\n",
    "rlm_loss = []\n",
    "rlm_best_tokenwise_acc = []\n",
    "rlm_best_loss = []\n",
    "all_losses = []\n",
    "\n",
    "dataset_gold_loss = []\n",
    "dataset_p_temp = 0\n",
    "rlm_temp=0.01\n",
    "rejection_sample = 100\n",
    "# pairs = get_reverse_pair(dataset['train'], lambda x, y: start_chunk_hf(x,y,num_prefix_tokens=10), tokenizer)\n",
    "\n",
    "for p,pair in enumerate(tqdm.tqdm(pairs)):\n",
    "    if len(rlm_loss)==10: break\n",
    "    if len(pair[0])<10 or len(pair[1])<10: continue\n",
    "    prefix_loss,suffix_loss = forward_loss(model, pair, tokenizer)\n",
    "    # if suffix_loss>2.1: continue #this is around 10th percentile of losses for 170m\n",
    "    prefix, suffix = pair\n",
    "    prefix_tokens = tokenizer.encode(prefix)\n",
    "    len_prefix = len(prefix_tokens)\n",
    "\n",
    "    min_loss, min_prefix = float('inf'), None\n",
    "    all_losses.append([])\n",
    "    for t in range(rejection_sample):\n",
    "        predicted_prefix = reverse_normalized_generate(bwd_model, tokenizer, suffix, len_prefix, None, temperature=rlm_temp) #1.425 at 0.25 partial Bayes update vs 1.437 at 0 i.e. default\n",
    "        predicted_prefix_tokens = tokenizer.encode(predicted_prefix)[:len_prefix]\n",
    "        predicted_prefix = tokenizer.decode(predicted_prefix_tokens)\n",
    "        predicted_prefix_loss, predicted_suffix_loss = forward_loss(model, (predicted_prefix, suffix), tokenizer)\n",
    "        all_losses[-1].append(predicted_suffix_loss.item())\n",
    "        if predicted_suffix_loss < min_loss:\n",
    "            min_loss = predicted_suffix_loss\n",
    "            min_prefix = predicted_prefix\n",
    "            min_prefix_tokens = predicted_prefix_tokens\n",
    "    print(f'True prefix is:\\n{prefix} \\n\\nPredicted prefix:\\n{min_prefix}\\nfor suffix:\\n {suffix}')\n",
    "    print(f'Loss for suffix given predicted prefix is {min_loss.item()} \\n Suffix loss for true prefix is {suffix_loss.item()}')\n",
    "    print(f'NLL on predicted prefix is {predicted_prefix_loss.item()} \\n NLL on true prefix is {prefix_loss.item()}')\n",
    "\n",
    "    dataset_gold_loss.append(suffix_loss)\n",
    "    rlm_tokenwise_acc.append(sum([1 for i in range(len(prefix_tokens)) if prefix_tokens[i] == predicted_prefix_tokens[i]])/len(prefix_tokens))\n",
    "    rlm_loss.append(predicted_suffix_loss.item())\n",
    "    rlm_best_tokenwise_acc.append(sum([1 for i in range(len(prefix_tokens)) if prefix_tokens[i] == min_prefix_tokens[i]])/len(prefix_tokens))\n",
    "    rlm_best_loss.append(min_loss.item())\n",
    "\n",
    "print(f'Average tokenwise accuracy is {sum(rlm_tokenwise_acc)/len(rlm_tokenwise_acc)}')\n",
    "print(f'Average loss is {sum(rlm_loss)/len(rlm_loss)}')\n",
    "print(f'Average dataset gold loss is {sum(dataset_gold_loss)/len(dataset_gold_loss)}')\n",
    "print(f'Best tokenwise accuracy is {sum(rlm_best_tokenwise_acc)/len(rlm_best_tokenwise_acc)}')\n",
    "print(f'Best loss is {sum(rlm_best_loss)/len(rlm_best_loss)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average loss is 3.585718604326248\n",
    "Average dataset gold loss is 3.3005855083465576\n",
    "\n",
    "3.2 with 0.25, 0\n",
    "Average loss is 2.9196928358078003\n",
    "Average dataset gold loss is 2.9133758544921875\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "                                                                                  [PUBLISH]\n",
      "\n",
      "\n",
      "                   \n",
      "\n",
      "Predicted prefix:\n",
      "           [DO NOT PUBLISH]\n",
      "\n",
      "\n",
      "                IN\n",
      "for suffix:\n",
      " IN THE UNITED STATES COURT OF APPEALS\n",
      "\n",
      "                            FOR THE ELEVENTH CIRC\n",
      "Loss for suffix given predicted prefix is 1.5578012466430664 \n",
      " Suffix loss for true prefix is 1.1715296506881714\n",
      "NLL on predicted prefix is 5.3471269607543945 \n",
      " NLL on true prefix is 3.7086830139160156\n",
      "True prefix is:\n",
      "Mystikal (album)\n",
      "\n",
      "My \n",
      "\n",
      "Predicted prefix:\n",
      " he said.Mystikal\n",
      "\n",
      "My\n",
      "for suffix:\n",
      " stikal is the eponymous self-titled debut studio album by American rapper Mystikal. It was independently self-released on June 14, 1994, by Big Boy Records. The\n",
      "Loss for suffix given predicted prefix is 2.3714146614074707 \n",
      " Suffix loss for true prefix is 2.054072141647339\n",
      "NLL on predicted prefix is 4.269046783447266 \n",
      " NLL on true prefix is 3.6076202392578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "5. Let b be (-105)/(-10)* \n",
      "\n",
      "Predicted prefix:\n",
      "*k. Let b be 4/2*\n",
      "for suffix:\n",
      " 6/k. Let y be (-3)/1 + (-196)/b. Solve -y = 4*u + 3*v, -2*v + 5*v = -2\n",
      "Loss for suffix given predicted prefix is 1.522289514541626 \n",
      " Suffix loss for true prefix is 1.646061658859253\n",
      "NLL on predicted prefix is 2.721585750579834 \n",
      " NLL on true prefix is 3.1036183834075928\n",
      "True prefix is:\n",
      "Hyperolius ferrugineus\n",
      "\n",
      "Hyper \n",
      "\n",
      "Predicted prefix:\n",
      "Hyperolius ferrugineus\n",
      "\n",
      "Hyper\n",
      "for suffix:\n",
      " olius ferrugineus is a species of frog in the family Hyperoliidae.\n",
      "It is endemic to Democratic Republic of the Congo.\n",
      "Its natural habitats are subtropical or tropical mois\n",
      "Loss for suffix given predicted prefix is 1.979470133781433 \n",
      " Suffix loss for true prefix is 1.979470133781433\n",
      "NLL on predicted prefix is 4.303708553314209 \n",
      " NLL on true prefix is 4.303708553314209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "Hatamabad, Markazi\n",
      "\n",
      "H \n",
      "\n",
      "Predicted prefix:\n",
      " he said.Jatamabad\n",
      "\n",
      "J\n",
      "for suffix:\n",
      " atamabad (, also Romanized as Ḩātamābād) is a village in Farmahin Rural District, in the Central District of Farahan County, Markazi\n",
      "Loss for suffix given predicted prefix is 1.5585622787475586 \n",
      " Suffix loss for true prefix is 1.4432446956634521\n",
      "NLL on predicted prefix is 4.541042804718018 \n",
      " NLL on true prefix is 4.4801740646362305\n",
      "True prefix is:\n",
      "<?php defined('BX_DOL') \n",
      "\n",
      "Predicted prefix:\n",
      "_BEFORE_HACK_PAUSE')\n",
      "for suffix:\n",
      "  or die('hack attempt');\n",
      "/**\n",
      " * Copyright (c) UNA, Inc - https://una.io\n",
      " * MIT License - https://opensource.org/licenses/MIT\n",
      "Loss for suffix given predicted prefix is 1.8252325057983398 \n",
      " Suffix loss for true prefix is 1.6047663688659668\n",
      "NLL on predicted prefix is 4.435699462890625 \n",
      " NLL on true prefix is 4.108913421630859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "<header class=\"header-wrapper\">\n",
      "\n",
      "   \n",
      "\n",
      "Predicted prefix:\n",
      ">\n",
      "<div class=\"container\">\n",
      "  \n",
      "for suffix:\n",
      " <nav class=\"inner\">\n",
      "    <div class=\"title\">\n",
      "      <a href=\"/\">\n",
      "        <img class=\"logo\" src=\"<%- url_for(theme.profile\n",
      "Loss for suffix given predicted prefix is 1.8682317733764648 \n",
      " Suffix loss for true prefix is 1.8162084817886353\n",
      "NLL on predicted prefix is 1.1058136224746704 \n",
      " NLL on true prefix is 2.780362367630005\n",
      "True prefix is:\n",
      "/*\n",
      "Copyright (C) 2011 Mark Chandler ( \n",
      "\n",
      "Predicted prefix:\n",
      " part of the Desura(R) project (\n",
      "for suffix:\n",
      " Desura Net Pty Ltd)\n",
      "Copyright (C) 2014 Bad Juju Games, Inc.\n",
      "\n",
      "This program is free software: you can redistribute it and/or modify\n",
      "it under the\n",
      "Loss for suffix given predicted prefix is 2.050963878631592 \n",
      " Suffix loss for true prefix is 1.5706523656845093\n",
      "NLL on predicted prefix is 4.815761566162109 \n",
      " NLL on true prefix is 4.146985054016113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "The present invention relates to catalyst components for the polymerization \n",
      "\n",
      "Predicted prefix:\n",
      ", to the use of said catalysts in the polymerization\n",
      "for suffix:\n",
      "  of olefins, to the catalysts obtained therefrom and to the use of said catalysts in the polymerization of olefins CH2xe2x95x\n",
      "Loss for suffix given predicted prefix is 1.7928210496902466 \n",
      " Suffix loss for true prefix is 1.878375768661499\n",
      "NLL on predicted prefix is 3.803551435470581 \n",
      " NLL on true prefix is 2.978477954864502\n",
      "True prefix is:\n",
      "1. Introduction {#sec1-ijerph-17 \n",
      "\n",
      "Predicted prefix:\n",
      "1. Introduction {#sec1-ijerph-17\n",
      "for suffix:\n",
      " -01067}\n",
      "===============\n",
      "\n",
      "Nasolacrimal duct obstruction (NLDO) is the most common cause of childhood epiphora \\[[@B1-ijerph-17-010\n",
      "Loss for suffix given predicted prefix is 1.7206493616104126 \n",
      " Suffix loss for true prefix is 1.7206493616104126\n",
      "NLL on predicted prefix is 1.8608165979385376 \n",
      " NLL on true prefix is 1.8608165979385376\n",
      "Average loss is 1.824743640422821\n"
     ]
    }
   ],
   "source": [
    "tokenwise_acc = []\n",
    "loss = []\n",
    "for p,pair in enumerate(pairs):\n",
    "    if len(loss)==100: break\n",
    "    if len(pair[0])<10 or len(pair[1])<10: continue\n",
    "    prefix_loss,suffix_loss = forward_loss(model, pair)\n",
    "    if suffix_loss>2.1: continue #this is around 10th percentile of losses for 170m\n",
    "    prefix, suffix = pair\n",
    "    len_prefix = len(tokenizer(prefix)['input_ids'])\n",
    "    predicted_prefix = reverse_generate(bwd_model, tokenizer, suffix, len_prefix)[0]\n",
    "    predicted_prefix = tokenizer.decode(tokenizer.encode(predicted_prefix)[:len_prefix])\n",
    "\n",
    "    predicted_prefix_loss, predicted_suffix_loss = forward_loss(model, (predicted_prefix, suffix))\n",
    "    print(f'True prefix is:\\n{prefix} \\n\\nPredicted prefix:\\n{predicted_prefix}\\nfor suffix:\\n {suffix}')\n",
    "    print(f'Loss for suffix given predicted prefix is {predicted_suffix_loss.item()} \\n Suffix loss for true prefix is {suffix_loss.item()}')\n",
    "    print(f'NLL on predicted prefix is {predicted_prefix_loss.item()} \\n NLL on true prefix is {prefix_loss.item()}')\n",
    "    loss.append(predicted_suffix_loss.item())\n",
    "# print(f'Average tokenwise accuracy is {sum(tokenwise_acc)/len(tokenwise_acc)}')\n",
    "print(f'Average loss is {sum(loss)/len(loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' dish towel and set it on the mat next to the kitchen.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import reverse_tokenize, reverse_decode\n",
    "def reverse_normalized_forward(reverse_model, tokenizer, target, normalizer):\n",
    "    inputs = reverse_tokenize(tokenizer, target)\n",
    "    outputs = reverse_model(inputs).logits[0,-1,:]\n",
    "    outputs = torch.nn.Softmax(dim=-1)(outputs).cpu()\n",
    "    outputs = torch.mul(outputs, normalizer)\n",
    "    return outputs\n",
    "\n",
    "def reverse_normalized_generate(reverse_model, tokenizer, target, max_length, normalizer, temperature=1):\n",
    "    prefix = []\n",
    "    for i in range(max_length):\n",
    "        normalized_probs = reverse_normalized_forward(reverse_model, tokenizer, ''.join(prefix[::-1]) + target, normalizer)\n",
    "        if not temperature:\n",
    "            token = tokenizer.decode(torch.argmax(normalized_probs))\n",
    "        else:\n",
    "            probs = torch.div(normalized_probs, temperature)\n",
    "            probs = torch.nn.Softmax(dim=-1)(probs)\n",
    "            token = tokenizer.decode(torch.multinomial(probs, num_samples=1))\n",
    "        if token == '[PAD]' or token == '[EOS]':\n",
    "            break\n",
    "        prefix.append(token)\n",
    "    return ''.join(prefix[::-1])+target\n",
    "\n",
    "inverse_dataset_probs = torch.reciprocal(dataset_probs)\n",
    "reverse_normalized_generate(bwd_model, tokenizer, ' on the mat next to the kitchen.', 5, inverse_dataset_probs**0.1, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss is 1.5034217107743024\n"
     ]
    }
   ],
   "source": [
    "tokenwise_acc = []\n",
    "loss = []\n",
    "for p,pair in enumerate(pairs):\n",
    "    if len(loss)==250: break\n",
    "    if len(pair[0])<10 or len(pair[1])<10: continue\n",
    "    prefix_loss,suffix_loss = forward_loss(model, pair)\n",
    "    # if suffix_loss>2.1: continue #this is around 10th percentile of losses for 170m\n",
    "    prefix, suffix = pair\n",
    "    len_prefix = len(tokenizer(prefix)['input_ids'])\n",
    "    predicted_prefix = reverse_normalized_generate(bwd_model, tokenizer, suffix, len_prefix, inverse_dataset_probs**0.25, temperature=0) #1.425 at 0.25 partial Bayes update vs 1.437 at 0 i.e. default\n",
    "    predicted_prefix = tokenizer.decode(tokenizer.encode(predicted_prefix)[:len_prefix])\n",
    "\n",
    "    predicted_prefix_loss, predicted_suffix_loss = forward_loss(model, (predicted_prefix, suffix))\n",
    "    # print(f'True prefix is:\\n{prefix} \\n\\n')\n",
    "    # print(f'Predicted prefix:\\n{predicted_prefix}\\nfor suffix:\\n {suffix}')\n",
    "    # print(f'Loss for suffix given predicted prefix is {predicted_suffix_loss.item()} \\n Suffix loss for true prefix is {suffix_loss.item()}')\n",
    "    # print(f'NLL on predicted prefix is {predicted_prefix_loss.item()} \\n NLL on true prefix is {prefix_loss.item()}')\n",
    "    loss.append(predicted_suffix_loss.item())\n",
    "# print(f'Average tokenwise accuracy is {sum(tokenwise_acc)/len(tokenwise_acc)}')\n",
    "print(f'Average loss is {sum(loss)/len(loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/jp6263/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 297.19it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_token_probabilities(tokenizer, dataset=\"NeelNanda/pile-10k\", vocab_size=50304):\n",
    "    data = load_dataset(dataset)\n",
    "    counts = torch.zeros(vocab_size, dtype=torch.float) #tokenizer.vocab_size is fake 50304 is the model output dimension which is what we care about\n",
    "\n",
    "    for chunk in data['train']:\n",
    "        # Extract text from chunk (assuming each chunk is a dictionary with a \"text\" key)\n",
    "        text = chunk['text']\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = tokenizer(text, return_tensors=\"pt\").input_ids[0]\n",
    "\n",
    "        # Count occurrences for each token\n",
    "        for tok in tokens:\n",
    "            counts[tok] += 1\n",
    "\n",
    "    # Normalize the counts to get probabilities\n",
    "    total_tokens = torch.sum(counts)\n",
    "    probabilities = counts / total_tokens\n",
    "    min_val = probabilities[probabilities > 0].min()\n",
    "    probabilities[probabilities == 0] = min_val\n",
    "    return probabilities\n",
    "\n",
    "dataset_probs = get_token_probabilities(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' it, and set it on the mat next to the kitchen.']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_generate(bwd_model, tokenizer, ' on the mat next to the kitchen.', 5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.4364e-05)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slacktokens",
   "language": "python",
   "name": "slacktokens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
