{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_list = os.chdir('./../reverse-dynamics-nlp/')\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, GPTNeoXForCausalLM\n",
    "from prompt_optimizer import PromptOptimizer\n",
    "from utils import reverse_normalized_generate, start_chunk_hf, forward_loss, reverse_normalized_forward, get_token_probabilities, rand_init, get_reverse_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pile-10k eval. Load data, backwards and forwards models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"NeelNanda/pile-10k\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"afterless/reverse-pythia-160m\")\n",
    "pairs = get_reverse_pair(dataset['train'], start_chunk_hf, tokenizer)\n",
    "print(next(pairs))\n",
    "bwd_model = GPTNeoXForCausalLM.from_pretrained(\"afterless/reverse-pythia-160m\").cuda()\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\", cache_dir='/scratch/jp6263/hf/models/').cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate GCG with forward LM-guided sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 5 #None for default GCG with uniform sampling\n",
    "prefix_probability_grad_weight = 0.1\n",
    "gcg = PromptOptimizer(model, tokenizer, n_proposals=128, n_epochs=250, n_top_indices=128, prefix_loss_weight=prefix_probability_grad_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcg_tokenwise_acc = []\n",
    "gcg_loss = []\n",
    "for p,pair in enumerate(pairs):\n",
    "    if len(gcg_loss)==100: break\n",
    "    if len(pair[0])<10 or len(pair[1])<10: continue\n",
    "    prefix_loss,suffix_loss = forward_loss(model, pair, tokenizer)\n",
    "    # if suffix_loss>2.1: continue #this is around 10th percentile of losses for 170m\n",
    "    prefix, suffix = pair\n",
    "    prefix_tokens = tokenizer.encode(prefix)\n",
    "    len_prefix = len(prefix_tokens)\n",
    "    rand_prefix = rand_init(len_prefix, tokenizer)\n",
    "    optimized_string = gcg.optimize(rand_prefix, suffix, temperature=temp)\n",
    "    predicted_prefix_tokens = tokenizer.encode(optimized_string)[:len_prefix]\n",
    "    predicted_prefix = tokenizer.decode(predicted_prefix_tokens)\n",
    "    predicted_prefix_loss, predicted_suffix_loss = forward_loss(model, (predicted_prefix, suffix), tokenizer)\n",
    "    # print(f'True prefix is:\\n{prefix} \\n\\nPredicted prefix:\\n{predicted_prefix}\\nfor suffix:\\n {suffix}')\n",
    "    # print(f'Loss for suffix given predicted prefix is {predicted_suffix_loss.item()} \\n Suffix loss for true prefix is {suffix_loss.item()}')\n",
    "    # print(f'NLL on predicted prefix is {predicted_prefix_loss.item()} \\n NLL on true prefix is {prefix_loss.item()}')\n",
    "    gcg_loss.append(predicted_suffix_loss.item())\n",
    "    gcg_tokenwise_acc.append(sum([1 for i in range(len(prefix_tokens)) if prefix_tokens[i] == predicted_prefix_tokens[i]])/len(prefix_tokens))\n",
    "print(f'Average tokenwise accuracy is {sum(gcg_tokenwise_acc)/len(gcg_tokenwise_acc)}')\n",
    "print(f'Average loss is {sum(gcg_loss)/len(gcg_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 2 #None for default GCG with uniform sampling\n",
    "prefix_probability_grad_weight = 0.25\n",
    "gcg = PromptOptimizer(model, tokenizer, n_proposals=128, n_epochs=250, n_top_indices=128, prefix_loss_weight=prefix_probability_grad_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcg_tokenwise_acc = []\n",
    "gcg_loss = []\n",
    "for p,pair in enumerate(pairs):\n",
    "    if len(gcg_loss)==100: break\n",
    "    if len(pair[0])<10 or len(pair[1])<10: continue\n",
    "    prefix_loss,suffix_loss = forward_loss(model, pair, tokenizer)\n",
    "    # if suffix_loss>2.1: continue #this is around 10th percentile of losses for 170m\n",
    "    prefix, suffix = pair\n",
    "    prefix_tokens = tokenizer.encode(prefix)\n",
    "    len_prefix = len(prefix_tokens)\n",
    "    rand_prefix = rand_init(len_prefix, tokenizer)\n",
    "    optimized_string = gcg.optimize(rand_prefix, suffix, temperature=temp)\n",
    "    predicted_prefix_tokens = tokenizer.encode(optimized_string)[:len_prefix]\n",
    "    predicted_prefix = tokenizer.decode(predicted_prefix_tokens)\n",
    "    predicted_prefix_loss, predicted_suffix_loss = forward_loss(model, (predicted_prefix, suffix), tokenizer)\n",
    "    # print(f'True prefix is:\\n{prefix} \\n\\nPredicted prefix:\\n{predicted_prefix}\\nfor suffix:\\n {suffix}')\n",
    "    # print(f'Loss for suffix given predicted prefix is {predicted_suffix_loss.item()} \\n Suffix loss for true prefix is {suffix_loss.item()}')\n",
    "    # print(f'NLL on predicted prefix is {predicted_prefix_loss.item()} \\n NLL on true prefix is {prefix_loss.item()}')\n",
    "    gcg_loss.append(predicted_suffix_loss.item())\n",
    "    gcg_tokenwise_acc.append(sum([1 for i in range(len(prefix_tokens)) if prefix_tokens[i] == predicted_prefix_tokens[i]])/len(prefix_tokens))\n",
    "print(f'Average tokenwise accuracy is {sum(gcg_tokenwise_acc)/len(gcg_tokenwise_acc)}')\n",
    "print(f'Average loss is {sum(gcg_loss)/len(gcg_loss)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load dataset probabilities and setup for reverse LM eval with p(p) normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_probs = get_token_probabilities(tokenizer)\n",
    "inverse_dataset_probs = torch.reciprocal(dataset_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_p_temp = 0\n",
    "rlm_temp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlm_tokenwise_acc = []\n",
    "rlm_loss = []\n",
    "for p,pair in enumerate(pairs):\n",
    "    if len(rlm_loss)==100: break\n",
    "    if len(pair[0])<10 or len(pair[1])<10: continue\n",
    "    prefix_loss,suffix_loss = forward_loss(model, pair, tokenizer)\n",
    "    # if suffix_loss>2.1: continue #this is around 10th percentile of losses for 170m\n",
    "    prefix, suffix = pair\n",
    "    prefix_tokens = tokenizer.encode(prefix)\n",
    "    len_prefix = len(prefix_tokens)\n",
    "    predicted_prefix = reverse_normalized_generate(bwd_model, tokenizer, suffix, len_prefix, inverse_dataset_probs**dataset_p_temp, temperature=rlm_temp) \n",
    "    predicted_prefix_tokens = tokenizer.encode(predicted_prefix)[:len_prefix]\n",
    "    predicted_prefix = tokenizer.decode(predicted_prefix_tokens)\n",
    "\n",
    "    predicted_prefix_loss, predicted_suffix_loss = forward_loss(model, (predicted_prefix, suffix), tokenizer)\n",
    "    # print(f'True prefix is:\\n{prefix} \\n\\nPredicted prefix:\\n{predicted_prefix}\\nfor suffix:\\n {suffix}')\n",
    "    # print(f'Loss for suffix given predicted prefix is {predicted_suffix_loss.item()} \\n Suffix loss for true prefix is {suffix_loss.item()}')\n",
    "    # print(f'NLL on predicted prefix is {predicted_prefix_loss.item()} \\n NLL on true prefix is {prefix_loss.item()}')\n",
    "\n",
    "    rlm_tokenwise_acc.append(sum([1 for i in range(len(prefix_tokens)) if prefix_tokens[i] == predicted_prefix_tokens[i]])/len(prefix_tokens))\n",
    "    rlm_loss.append(predicted_suffix_loss.item())\n",
    "\n",
    "print(f'Average tokenwise accuracy is {sum(rlm_tokenwise_acc)/len(rlm_tokenwise_acc)}')\n",
    "print(f'Average loss is {sum(rlm_loss)/len(rlm_loss)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate rejection sampling of RLM (no normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlm_tokenwise_acc = []\n",
    "rlm_loss = []\n",
    "rlm_best_tokenwise_acc = []\n",
    "rlm_best_loss = []\n",
    "all_losses = []\n",
    "rlm_greedy_loss = []\n",
    "\n",
    "dataset_gold_loss = []\n",
    "dataset_p_temp = 0\n",
    "rlm_temp=0.01\n",
    "rejection_sample = 100\n",
    "eval_size=100\n",
    "pairs = get_reverse_pair(dataset['train'], lambda x, y: start_chunk_hf(x,y,num_prefix_tokens=10), tokenizer)\n",
    "\n",
    "for p,pair in enumerate(tqdm(pairs)):\n",
    "    if len(rlm_loss)==eval_size: break\n",
    "    if len(pair[0])<10 or len(pair[1])<10: continue\n",
    "    prefix_loss,suffix_loss = forward_loss(model, pair, tokenizer)\n",
    "    # if suffix_loss>2.1: continue #this is around 10th percentile of losses for 170m\n",
    "    prefix, suffix = pair\n",
    "    prefix_tokens = tokenizer.encode(prefix)\n",
    "    len_prefix = len(prefix_tokens)\n",
    "\n",
    "    min_loss, min_prefix = float('inf'), None\n",
    "    all_losses.append([])\n",
    "    for t in range(rejection_sample):\n",
    "        predicted_prefix = reverse_normalized_generate(bwd_model, tokenizer, suffix, len_prefix, None, temperature=rlm_temp) \n",
    "        predicted_prefix_tokens = tokenizer.encode(predicted_prefix)[:len_prefix]\n",
    "        predicted_prefix = tokenizer.decode(predicted_prefix_tokens)\n",
    "        predicted_prefix_loss, predicted_suffix_loss = forward_loss(model, (predicted_prefix, suffix), tokenizer)\n",
    "        all_losses[-1].append(predicted_suffix_loss.item())\n",
    "        if predicted_suffix_loss < min_loss:\n",
    "            min_loss = predicted_suffix_loss\n",
    "            min_prefix = predicted_prefix\n",
    "            min_prefix_tokens = predicted_prefix_tokens\n",
    "    # print(f'True prefix is:\\n{prefix} \\n\\nPredicted prefix:\\n{min_prefix}\\nfor suffix:\\n {suffix}')\n",
    "    # print(f'Loss for suffix given predicted prefix is {min_loss.item()} \\n Suffix loss for true prefix is {suffix_loss.item()}')\n",
    "    # print(f'NLL on predicted prefix is {predicted_prefix_loss.item()} \\n NLL on true prefix is {prefix_loss.item()}')\n",
    "\n",
    "    #Now get greedy loss as baseline\n",
    "\n",
    "    predicted_prefix = reverse_normalized_generate(bwd_model, tokenizer, suffix, len_prefix, None, temperature=0) \n",
    "    predicted_prefix_tokens = tokenizer.encode(predicted_prefix)[:len_prefix]\n",
    "    predicted_prefix = tokenizer.decode(predicted_prefix_tokens)\n",
    "    _, greedy_loss = forward_loss(model, (predicted_prefix, suffix), tokenizer)\n",
    "\n",
    "    dataset_gold_loss.append(suffix_loss.item())\n",
    "    rlm_tokenwise_acc.append(sum([1 for i in range(len(prefix_tokens)) if prefix_tokens[i] == predicted_prefix_tokens[i]])/len(prefix_tokens))\n",
    "    rlm_loss.append(predicted_suffix_loss.item())\n",
    "    rlm_best_tokenwise_acc.append(sum([1 for i in range(len(prefix_tokens)) if prefix_tokens[i] == min_prefix_tokens[i]])/len(prefix_tokens))\n",
    "    rlm_best_loss.append(min_loss.item())\n",
    "    rlm_greedy_loss.append(greedy_loss.item())\n",
    "\n",
    "print(f'Average tokenwise accuracy is {sum(rlm_tokenwise_acc)/len(rlm_tokenwise_acc)}')\n",
    "print(f'Average loss is {sum(rlm_loss)/len(rlm_loss)}')\n",
    "print(f'Average dataset gold loss is {sum(dataset_gold_loss)/len(dataset_gold_loss)}')\n",
    "print(f'Best tokenwise accuracy is {sum(rlm_best_tokenwise_acc)/len(rlm_best_tokenwise_acc)}')\n",
    "print(f'Best loss is {sum(rlm_best_loss)/len(rlm_best_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialization\n",
    "Ns = range(1, rejection_sample)\n",
    "mean_best_of_N_loss = []\n",
    "\n",
    "for N in Ns:\n",
    "    best_of_N_loss = [min(single_list[:N]) for single_list in all_losses]\n",
    "    mean_best_of_N_loss.append(np.mean(best_of_N_loss))\n",
    "\n",
    "plt.axhline(y=sum(dataset_gold_loss)/len(dataset_gold_loss), color='r', linestyle='--', label='Loss given true prefix')\n",
    "plt.axhline(y=sum(rlm_greedy_loss)/len(rlm_greedy_loss), color='g', linestyle='--', label='Loss given greedy decode prefix')\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.plot(Ns, mean_best_of_N_loss, marker='o')\n",
    "plt.xlabel('Number of Rejection Sampling Steps')\n",
    "plt.ylabel('Arithmetic Mean of Best-of-N Loss')\n",
    "plt.title('Arithmetic Mean of Best-of-N Loss vs Rejection Sampling Steps')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
